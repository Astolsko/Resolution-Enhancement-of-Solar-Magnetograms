{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import random\n",
    "\n",
    "# from our codebase\n",
    "from conv_layer import conv_layer\n",
    "from RLFB import RLFB\n",
    "from SUBP import SubPixelConvBlock  \n",
    "from Trainning_Loop import train_model, CharbonnierLoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MESR(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, num_blocks=12, esa_channels=16, upscale_factor=2):\n",
    "        super(MESR, self).__init__()\n",
    "\n",
    "        self.conv_in = conv_layer(in_channels, mid_channels, 3)\n",
    "        self.RLFB_blocks = nn.Sequential(*[RLFB(mid_channels, esa_channels=esa_channels) for _ in range(num_blocks)])\n",
    "        self.conv_out = conv_layer(mid_channels, out_channels, 3)\n",
    "        self.sub_pixel_conv = SubPixelConvBlock(out_channels, out_channels, upscale_factor=upscale_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_conv_in = self.conv_in(x)  \n",
    "        out_RLFB = self.RLFB_blocks(out_conv_in)  \n",
    "        out_skip = out_RLFB + out_conv_in  \n",
    "        out = self.conv_out(out_skip)  \n",
    "        out = self.sub_pixel_conv(out)  \n",
    "        return out\n",
    "\n",
    "\n",
    "def model_summary(model, device):\n",
    "    model.to(device)\n",
    "    summary(model, input_size=(3, 256, 256)) # Change order & num of channels to match grayscale channel \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, lr_transform=None, hr_transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_images = os.listdir(lr_dir)\n",
    "        self.lr_transform = lr_transform\n",
    "        self.hr_transform = hr_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_image_path = os.path.join(self.lr_dir, self.lr_images[idx])\n",
    "        hr_image_path = os.path.join(self.hr_dir, self.lr_images[idx])\n",
    "\n",
    "        lr_image = Image.open(lr_image_path).convert(\"RGB\")\n",
    "        hr_image = Image.open(hr_image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.lr_transform:\n",
    "            lr_image = self.lr_transform(lr_image)\n",
    "        if self.hr_transform:\n",
    "            hr_image = self.hr_transform(hr_image)\n",
    "\n",
    "        return {'image': lr_image, 'label': hr_image}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloaders(train_dataset, val_dataset, batch_size=2): # setting the batch size to 2\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def setup_training(model, device, train_loader, val_loader, epochs=200, patience=50):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    loss_function = CharbonnierLoss(epsilon=1e-6)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss_function,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        val_interval=10,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        output_dir=\"./model_output\"  # Specify the output directory\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of available GPUs: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA RTX A6000\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           1,792\n",
      "            Conv2d-2         [-1, 64, 256, 256]          36,928\n",
      "              SiLU-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 256, 256]          36,928\n",
      "              SiLU-5         [-1, 64, 256, 256]               0\n",
      "            Conv2d-6         [-1, 64, 256, 256]          36,928\n",
      "              SiLU-7         [-1, 64, 256, 256]               0\n",
      "            Conv2d-8         [-1, 64, 256, 256]           4,160\n",
      "            Conv2d-9         [-1, 16, 256, 256]           1,040\n",
      "           Conv2d-10         [-1, 16, 127, 127]           2,320\n",
      "           Conv2d-11           [-1, 16, 41, 41]           2,320\n",
      "           Conv2d-12         [-1, 16, 256, 256]             272\n",
      "           Conv2d-13         [-1, 64, 256, 256]           1,088\n",
      "          Sigmoid-14         [-1, 64, 256, 256]               0\n",
      "              ESA-15         [-1, 64, 256, 256]               0\n",
      "             RLFB-16         [-1, 64, 256, 256]               0\n",
      "           Conv2d-17         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-18         [-1, 64, 256, 256]               0\n",
      "           Conv2d-19         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-20         [-1, 64, 256, 256]               0\n",
      "           Conv2d-21         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-22         [-1, 64, 256, 256]               0\n",
      "           Conv2d-23         [-1, 64, 256, 256]           4,160\n",
      "           Conv2d-24         [-1, 16, 256, 256]           1,040\n",
      "           Conv2d-25         [-1, 16, 127, 127]           2,320\n",
      "           Conv2d-26           [-1, 16, 41, 41]           2,320\n",
      "           Conv2d-27         [-1, 16, 256, 256]             272\n",
      "           Conv2d-28         [-1, 64, 256, 256]           1,088\n",
      "          Sigmoid-29         [-1, 64, 256, 256]               0\n",
      "              ESA-30         [-1, 64, 256, 256]               0\n",
      "             RLFB-31         [-1, 64, 256, 256]               0\n",
      "           Conv2d-32         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-33         [-1, 64, 256, 256]               0\n",
      "           Conv2d-34         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-35         [-1, 64, 256, 256]               0\n",
      "           Conv2d-36         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-37         [-1, 64, 256, 256]               0\n",
      "           Conv2d-38         [-1, 64, 256, 256]           4,160\n",
      "           Conv2d-39         [-1, 16, 256, 256]           1,040\n",
      "           Conv2d-40         [-1, 16, 127, 127]           2,320\n",
      "           Conv2d-41           [-1, 16, 41, 41]           2,320\n",
      "           Conv2d-42         [-1, 16, 256, 256]             272\n",
      "           Conv2d-43         [-1, 64, 256, 256]           1,088\n",
      "          Sigmoid-44         [-1, 64, 256, 256]               0\n",
      "              ESA-45         [-1, 64, 256, 256]               0\n",
      "             RLFB-46         [-1, 64, 256, 256]               0\n",
      "           Conv2d-47         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-48         [-1, 64, 256, 256]               0\n",
      "           Conv2d-49         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-50         [-1, 64, 256, 256]               0\n",
      "           Conv2d-51         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-52         [-1, 64, 256, 256]               0\n",
      "           Conv2d-53         [-1, 64, 256, 256]           4,160\n",
      "           Conv2d-54         [-1, 16, 256, 256]           1,040\n",
      "           Conv2d-55         [-1, 16, 127, 127]           2,320\n",
      "           Conv2d-56           [-1, 16, 41, 41]           2,320\n",
      "           Conv2d-57         [-1, 16, 256, 256]             272\n",
      "           Conv2d-58         [-1, 64, 256, 256]           1,088\n",
      "          Sigmoid-59         [-1, 64, 256, 256]               0\n",
      "              ESA-60         [-1, 64, 256, 256]               0\n",
      "             RLFB-61         [-1, 64, 256, 256]               0\n",
      "           Conv2d-62         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-63         [-1, 64, 256, 256]               0\n",
      "           Conv2d-64         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-65         [-1, 64, 256, 256]               0\n",
      "           Conv2d-66         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-67         [-1, 64, 256, 256]               0\n",
      "           Conv2d-68         [-1, 64, 256, 256]           4,160\n",
      "           Conv2d-69         [-1, 16, 256, 256]           1,040\n",
      "           Conv2d-70         [-1, 16, 127, 127]           2,320\n",
      "           Conv2d-71           [-1, 16, 41, 41]           2,320\n",
      "           Conv2d-72         [-1, 16, 256, 256]             272\n",
      "           Conv2d-73         [-1, 64, 256, 256]           1,088\n",
      "          Sigmoid-74         [-1, 64, 256, 256]               0\n",
      "              ESA-75         [-1, 64, 256, 256]               0\n",
      "             RLFB-76         [-1, 64, 256, 256]               0\n",
      "           Conv2d-77         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-78         [-1, 64, 256, 256]               0\n",
      "           Conv2d-79         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-80         [-1, 64, 256, 256]               0\n",
      "           Conv2d-81         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-82         [-1, 64, 256, 256]               0\n",
      "           Conv2d-83         [-1, 64, 256, 256]           4,160\n",
      "           Conv2d-84         [-1, 16, 256, 256]           1,040\n",
      "           Conv2d-85         [-1, 16, 127, 127]           2,320\n",
      "           Conv2d-86           [-1, 16, 41, 41]           2,320\n",
      "           Conv2d-87         [-1, 16, 256, 256]             272\n",
      "           Conv2d-88         [-1, 64, 256, 256]           1,088\n",
      "          Sigmoid-89         [-1, 64, 256, 256]               0\n",
      "              ESA-90         [-1, 64, 256, 256]               0\n",
      "             RLFB-91         [-1, 64, 256, 256]               0\n",
      "           Conv2d-92         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-93         [-1, 64, 256, 256]               0\n",
      "           Conv2d-94         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-95         [-1, 64, 256, 256]               0\n",
      "           Conv2d-96         [-1, 64, 256, 256]          36,928\n",
      "             SiLU-97         [-1, 64, 256, 256]               0\n",
      "           Conv2d-98         [-1, 64, 256, 256]           4,160\n",
      "           Conv2d-99         [-1, 16, 256, 256]           1,040\n",
      "          Conv2d-100         [-1, 16, 127, 127]           2,320\n",
      "          Conv2d-101           [-1, 16, 41, 41]           2,320\n",
      "          Conv2d-102         [-1, 16, 256, 256]             272\n",
      "          Conv2d-103         [-1, 64, 256, 256]           1,088\n",
      "         Sigmoid-104         [-1, 64, 256, 256]               0\n",
      "             ESA-105         [-1, 64, 256, 256]               0\n",
      "            RLFB-106         [-1, 64, 256, 256]               0\n",
      "          Conv2d-107         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-108         [-1, 64, 256, 256]               0\n",
      "          Conv2d-109         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-110         [-1, 64, 256, 256]               0\n",
      "          Conv2d-111         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-112         [-1, 64, 256, 256]               0\n",
      "          Conv2d-113         [-1, 64, 256, 256]           4,160\n",
      "          Conv2d-114         [-1, 16, 256, 256]           1,040\n",
      "          Conv2d-115         [-1, 16, 127, 127]           2,320\n",
      "          Conv2d-116           [-1, 16, 41, 41]           2,320\n",
      "          Conv2d-117         [-1, 16, 256, 256]             272\n",
      "          Conv2d-118         [-1, 64, 256, 256]           1,088\n",
      "         Sigmoid-119         [-1, 64, 256, 256]               0\n",
      "             ESA-120         [-1, 64, 256, 256]               0\n",
      "            RLFB-121         [-1, 64, 256, 256]               0\n",
      "          Conv2d-122         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-123         [-1, 64, 256, 256]               0\n",
      "          Conv2d-124         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-125         [-1, 64, 256, 256]               0\n",
      "          Conv2d-126         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-127         [-1, 64, 256, 256]               0\n",
      "          Conv2d-128         [-1, 64, 256, 256]           4,160\n",
      "          Conv2d-129         [-1, 16, 256, 256]           1,040\n",
      "          Conv2d-130         [-1, 16, 127, 127]           2,320\n",
      "          Conv2d-131           [-1, 16, 41, 41]           2,320\n",
      "          Conv2d-132         [-1, 16, 256, 256]             272\n",
      "          Conv2d-133         [-1, 64, 256, 256]           1,088\n",
      "         Sigmoid-134         [-1, 64, 256, 256]               0\n",
      "             ESA-135         [-1, 64, 256, 256]               0\n",
      "            RLFB-136         [-1, 64, 256, 256]               0\n",
      "          Conv2d-137         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-138         [-1, 64, 256, 256]               0\n",
      "          Conv2d-139         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-140         [-1, 64, 256, 256]               0\n",
      "          Conv2d-141         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-142         [-1, 64, 256, 256]               0\n",
      "          Conv2d-143         [-1, 64, 256, 256]           4,160\n",
      "          Conv2d-144         [-1, 16, 256, 256]           1,040\n",
      "          Conv2d-145         [-1, 16, 127, 127]           2,320\n",
      "          Conv2d-146           [-1, 16, 41, 41]           2,320\n",
      "          Conv2d-147         [-1, 16, 256, 256]             272\n",
      "          Conv2d-148         [-1, 64, 256, 256]           1,088\n",
      "         Sigmoid-149         [-1, 64, 256, 256]               0\n",
      "             ESA-150         [-1, 64, 256, 256]               0\n",
      "            RLFB-151         [-1, 64, 256, 256]               0\n",
      "          Conv2d-152         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-153         [-1, 64, 256, 256]               0\n",
      "          Conv2d-154         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-155         [-1, 64, 256, 256]               0\n",
      "          Conv2d-156         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-157         [-1, 64, 256, 256]               0\n",
      "          Conv2d-158         [-1, 64, 256, 256]           4,160\n",
      "          Conv2d-159         [-1, 16, 256, 256]           1,040\n",
      "          Conv2d-160         [-1, 16, 127, 127]           2,320\n",
      "          Conv2d-161           [-1, 16, 41, 41]           2,320\n",
      "          Conv2d-162         [-1, 16, 256, 256]             272\n",
      "          Conv2d-163         [-1, 64, 256, 256]           1,088\n",
      "         Sigmoid-164         [-1, 64, 256, 256]               0\n",
      "             ESA-165         [-1, 64, 256, 256]               0\n",
      "            RLFB-166         [-1, 64, 256, 256]               0\n",
      "          Conv2d-167         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-168         [-1, 64, 256, 256]               0\n",
      "          Conv2d-169         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-170         [-1, 64, 256, 256]               0\n",
      "          Conv2d-171         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-172         [-1, 64, 256, 256]               0\n",
      "          Conv2d-173         [-1, 64, 256, 256]           4,160\n",
      "          Conv2d-174         [-1, 16, 256, 256]           1,040\n",
      "          Conv2d-175         [-1, 16, 127, 127]           2,320\n",
      "          Conv2d-176           [-1, 16, 41, 41]           2,320\n",
      "          Conv2d-177         [-1, 16, 256, 256]             272\n",
      "          Conv2d-178         [-1, 64, 256, 256]           1,088\n",
      "         Sigmoid-179         [-1, 64, 256, 256]               0\n",
      "             ESA-180         [-1, 64, 256, 256]               0\n",
      "            RLFB-181         [-1, 64, 256, 256]               0\n",
      "          Conv2d-182         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-183         [-1, 64, 256, 256]               0\n",
      "          Conv2d-184         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-185         [-1, 64, 256, 256]               0\n",
      "          Conv2d-186         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-187         [-1, 64, 256, 256]               0\n",
      "          Conv2d-188         [-1, 64, 256, 256]           4,160\n",
      "          Conv2d-189         [-1, 16, 256, 256]           1,040\n",
      "          Conv2d-190         [-1, 16, 127, 127]           2,320\n",
      "          Conv2d-191           [-1, 16, 41, 41]           2,320\n",
      "          Conv2d-192         [-1, 16, 256, 256]             272\n",
      "          Conv2d-193         [-1, 64, 256, 256]           1,088\n",
      "         Sigmoid-194         [-1, 64, 256, 256]               0\n",
      "             ESA-195         [-1, 64, 256, 256]               0\n",
      "            RLFB-196         [-1, 64, 256, 256]               0\n",
      "          Conv2d-197         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-198         [-1, 64, 256, 256]               0\n",
      "          Conv2d-199         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-200         [-1, 64, 256, 256]               0\n",
      "          Conv2d-201         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-202         [-1, 64, 256, 256]               0\n",
      "          Conv2d-203         [-1, 64, 256, 256]           4,160\n",
      "          Conv2d-204         [-1, 16, 256, 256]           1,040\n",
      "          Conv2d-205         [-1, 16, 127, 127]           2,320\n",
      "          Conv2d-206           [-1, 16, 41, 41]           2,320\n",
      "          Conv2d-207         [-1, 16, 256, 256]             272\n",
      "          Conv2d-208         [-1, 64, 256, 256]           1,088\n",
      "         Sigmoid-209         [-1, 64, 256, 256]               0\n",
      "             ESA-210         [-1, 64, 256, 256]               0\n",
      "            RLFB-211         [-1, 64, 256, 256]               0\n",
      "          Conv2d-212         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-213         [-1, 64, 256, 256]               0\n",
      "          Conv2d-214         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-215         [-1, 64, 256, 256]               0\n",
      "          Conv2d-216         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-217         [-1, 64, 256, 256]               0\n",
      "          Conv2d-218         [-1, 64, 256, 256]           4,160\n",
      "          Conv2d-219         [-1, 16, 256, 256]           1,040\n",
      "          Conv2d-220         [-1, 16, 127, 127]           2,320\n",
      "          Conv2d-221           [-1, 16, 41, 41]           2,320\n",
      "          Conv2d-222         [-1, 16, 256, 256]             272\n",
      "          Conv2d-223         [-1, 64, 256, 256]           1,088\n",
      "         Sigmoid-224         [-1, 64, 256, 256]               0\n",
      "             ESA-225         [-1, 64, 256, 256]               0\n",
      "            RLFB-226         [-1, 64, 256, 256]               0\n",
      "          Conv2d-227         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-228         [-1, 64, 256, 256]               0\n",
      "          Conv2d-229         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-230         [-1, 64, 256, 256]               0\n",
      "          Conv2d-231         [-1, 64, 256, 256]          36,928\n",
      "            SiLU-232         [-1, 64, 256, 256]               0\n",
      "          Conv2d-233         [-1, 64, 256, 256]           4,160\n",
      "          Conv2d-234         [-1, 16, 256, 256]           1,040\n",
      "          Conv2d-235         [-1, 16, 127, 127]           2,320\n",
      "          Conv2d-236           [-1, 16, 41, 41]           2,320\n",
      "          Conv2d-237         [-1, 16, 256, 256]             272\n",
      "          Conv2d-238         [-1, 64, 256, 256]           1,088\n",
      "         Sigmoid-239         [-1, 64, 256, 256]               0\n",
      "             ESA-240         [-1, 64, 256, 256]               0\n",
      "            RLFB-241         [-1, 64, 256, 256]               0\n",
      "          Conv2d-242          [-1, 3, 256, 256]           1,731\n",
      "          Conv2d-243         [-1, 12, 256, 256]             336\n",
      "    PixelShuffle-244          [-1, 3, 512, 512]               0\n",
      "            ReLU-245          [-1, 3, 512, 512]               0\n",
      "SubPixelConvBlock-246          [-1, 3, 512, 512]               0\n",
      "================================================================\n",
      "Total params: 1,955,603\n",
      "Trainable params: 1,955,603\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 5980.29\n",
      "Params size (MB): 7.46\n",
      "Estimated Total Size (MB): 5988.50\n",
      "----------------------------------------------------------------\n",
      "----------\n",
      "epoch 1/200\n",
      "Step 1/182, train_loss: 0.3800, step time: 0.1263 sec\n",
      "Step 2/182, train_loss: 0.3872, step time: 0.1244 sec\n",
      "Step 3/182, train_loss: 0.4020, step time: 0.1257 sec\n",
      "Step 4/182, train_loss: 0.3798, step time: 0.1233 sec\n",
      "Step 5/182, train_loss: 0.3768, step time: 0.1274 sec\n",
      "Step 6/182, train_loss: 0.3887, step time: 0.1256 sec\n",
      "Step 7/182, train_loss: 0.3794, step time: 0.1200 sec\n",
      "Step 8/182, train_loss: 0.3695, step time: 0.1179 sec\n",
      "Step 9/182, train_loss: 0.3854, step time: 0.1160 sec\n",
      "Step 10/182, train_loss: 0.3812, step time: 0.1171 sec\n",
      "Step 11/182, train_loss: 0.3753, step time: 0.1168 sec\n",
      "Step 12/182, train_loss: 0.3916, step time: 0.1160 sec\n",
      "Step 13/182, train_loss: 0.3840, step time: 0.1152 sec\n",
      "Step 14/182, train_loss: 0.3866, step time: 0.1154 sec\n",
      "Step 15/182, train_loss: 0.3899, step time: 0.1167 sec\n",
      "Step 16/182, train_loss: 0.3957, step time: 0.1157 sec\n",
      "Step 17/182, train_loss: 0.3910, step time: 0.1142 sec\n",
      "Step 18/182, train_loss: 0.3857, step time: 0.1151 sec\n",
      "Step 19/182, train_loss: 0.3823, step time: 0.1163 sec\n",
      "Step 20/182, train_loss: 0.3825, step time: 0.1163 sec\n",
      "Step 21/182, train_loss: 0.3919, step time: 0.1151 sec\n",
      "Step 22/182, train_loss: 0.3838, step time: 0.1190 sec\n",
      "Step 23/182, train_loss: 0.3851, step time: 0.1179 sec\n",
      "Step 24/182, train_loss: 0.3822, step time: 0.1194 sec\n",
      "Step 25/182, train_loss: 0.3782, step time: 0.1177 sec\n",
      "Step 26/182, train_loss: 0.3848, step time: 0.1159 sec\n",
      "Step 27/182, train_loss: 0.3734, step time: 0.1166 sec\n",
      "Step 28/182, train_loss: 0.3789, step time: 0.1283 sec\n",
      "Step 29/182, train_loss: 0.3924, step time: 0.1301 sec\n",
      "Step 30/182, train_loss: 0.3840, step time: 0.1251 sec\n",
      "Step 31/182, train_loss: 0.3830, step time: 0.1218 sec\n",
      "Step 32/182, train_loss: 0.3856, step time: 0.1234 sec\n",
      "Step 33/182, train_loss: 0.3802, step time: 0.1248 sec\n",
      "Step 34/182, train_loss: 0.3754, step time: 0.1258 sec\n",
      "Step 35/182, train_loss: 0.3743, step time: 0.1231 sec\n",
      "Step 36/182, train_loss: 0.3796, step time: 0.1220 sec\n",
      "Step 37/182, train_loss: 0.3785, step time: 0.1245 sec\n",
      "Step 38/182, train_loss: 0.3866, step time: 0.1220 sec\n",
      "Step 39/182, train_loss: 0.3786, step time: 0.1221 sec\n",
      "Step 40/182, train_loss: 0.3835, step time: 0.1233 sec\n",
      "Step 41/182, train_loss: 0.3807, step time: 0.1245 sec\n",
      "Step 42/182, train_loss: 0.3789, step time: 0.1238 sec\n",
      "Step 43/182, train_loss: 0.3863, step time: 0.1235 sec\n",
      "Step 44/182, train_loss: 0.3668, step time: 0.1238 sec\n",
      "Step 45/182, train_loss: 0.3794, step time: 0.1222 sec\n",
      "Step 46/182, train_loss: 0.3848, step time: 0.1231 sec\n",
      "Step 47/182, train_loss: 0.3840, step time: 0.1218 sec\n",
      "Step 48/182, train_loss: 0.3716, step time: 0.1287 sec\n",
      "Step 49/182, train_loss: 0.3734, step time: 0.1275 sec\n",
      "Step 50/182, train_loss: 0.3768, step time: 0.1181 sec\n",
      "Step 51/182, train_loss: 0.3730, step time: 0.1161 sec\n",
      "Step 52/182, train_loss: 0.3772, step time: 0.1160 sec\n",
      "Step 53/182, train_loss: 0.3707, step time: 0.1168 sec\n",
      "Step 54/182, train_loss: 0.3819, step time: 0.1155 sec\n",
      "Step 55/182, train_loss: 0.3804, step time: 0.1148 sec\n",
      "Step 56/182, train_loss: 0.3862, step time: 0.1160 sec\n",
      "Step 57/182, train_loss: 0.3740, step time: 0.1164 sec\n",
      "Step 58/182, train_loss: 0.3793, step time: 0.1174 sec\n",
      "Step 59/182, train_loss: 0.3808, step time: 0.1147 sec\n",
      "Step 60/182, train_loss: 0.3844, step time: 0.1159 sec\n",
      "Step 61/182, train_loss: 0.3748, step time: 0.1174 sec\n",
      "Step 62/182, train_loss: 0.3654, step time: 0.1165 sec\n",
      "Step 63/182, train_loss: 0.3802, step time: 0.1164 sec\n",
      "Step 64/182, train_loss: 0.3859, step time: 0.1171 sec\n",
      "Step 65/182, train_loss: 0.4935, step time: 0.1152 sec\n",
      "Step 66/182, train_loss: 0.3801, step time: 0.1166 sec\n",
      "Step 67/182, train_loss: 0.3831, step time: 0.1164 sec\n",
      "Step 68/182, train_loss: 0.3709, step time: 0.1149 sec\n",
      "Step 69/182, train_loss: 0.3804, step time: 0.1184 sec\n",
      "Step 70/182, train_loss: 0.3771, step time: 0.1882 sec\n",
      "Step 71/182, train_loss: 0.3854, step time: 0.1159 sec\n",
      "Step 72/182, train_loss: 0.3841, step time: 0.1166 sec\n",
      "Step 73/182, train_loss: 0.3769, step time: 0.1159 sec\n",
      "Step 74/182, train_loss: 0.3797, step time: 0.1151 sec\n",
      "Step 75/182, train_loss: 0.3806, step time: 0.1167 sec\n",
      "Step 76/182, train_loss: 0.3698, step time: 0.1182 sec\n",
      "Step 77/182, train_loss: 0.3694, step time: 0.1162 sec\n",
      "Step 78/182, train_loss: 0.3767, step time: 0.1166 sec\n",
      "Step 79/182, train_loss: 0.3786, step time: 0.1167 sec\n",
      "Step 80/182, train_loss: 0.3833, step time: 0.1163 sec\n",
      "Step 81/182, train_loss: 0.3740, step time: 0.1147 sec\n",
      "Step 82/182, train_loss: 0.3820, step time: 0.1166 sec\n",
      "Step 83/182, train_loss: 0.3776, step time: 0.1163 sec\n",
      "Step 84/182, train_loss: 0.3815, step time: 0.1160 sec\n",
      "Step 85/182, train_loss: 0.3786, step time: 0.1160 sec\n",
      "Step 86/182, train_loss: 0.3768, step time: 0.1157 sec\n",
      "Step 87/182, train_loss: 0.3683, step time: 0.1166 sec\n",
      "Step 88/182, train_loss: 0.4796, step time: 0.1163 sec\n",
      "Step 89/182, train_loss: 0.3796, step time: 0.1153 sec\n",
      "Step 90/182, train_loss: 0.3879, step time: 0.1162 sec\n",
      "Step 91/182, train_loss: 0.3698, step time: 0.2317 sec\n",
      "Step 92/182, train_loss: 0.3720, step time: 0.1145 sec\n",
      "Step 93/182, train_loss: 0.3659, step time: 0.1169 sec\n",
      "Step 94/182, train_loss: 0.3856, step time: 0.1145 sec\n",
      "Step 95/182, train_loss: 0.3827, step time: 0.1154 sec\n",
      "Step 96/182, train_loss: 0.3734, step time: 0.1165 sec\n",
      "Step 97/182, train_loss: 0.3767, step time: 0.1154 sec\n",
      "Step 98/182, train_loss: 0.3868, step time: 0.2060 sec\n",
      "Step 99/182, train_loss: 0.3645, step time: 0.1149 sec\n",
      "Step 100/182, train_loss: 0.3832, step time: 0.1161 sec\n",
      "Step 101/182, train_loss: 0.3768, step time: 0.1154 sec\n",
      "Step 102/182, train_loss: 0.3718, step time: 0.1162 sec\n",
      "Step 103/182, train_loss: 0.3676, step time: 0.1150 sec\n",
      "Step 104/182, train_loss: 0.3812, step time: 0.1161 sec\n",
      "Step 105/182, train_loss: 0.3775, step time: 0.1146 sec\n",
      "Step 106/182, train_loss: 0.3823, step time: 0.1156 sec\n",
      "Step 107/182, train_loss: 0.3638, step time: 0.1143 sec\n",
      "Step 108/182, train_loss: 0.3727, step time: 0.1169 sec\n",
      "Step 109/182, train_loss: 0.3806, step time: 0.1154 sec\n",
      "Step 110/182, train_loss: 0.4894, step time: 0.1148 sec\n",
      "Step 111/182, train_loss: 0.3698, step time: 0.1153 sec\n",
      "Step 112/182, train_loss: 0.3748, step time: 0.1168 sec\n",
      "Step 113/182, train_loss: 0.3684, step time: 0.1152 sec\n",
      "Step 114/182, train_loss: 0.3777, step time: 0.1156 sec\n",
      "Step 115/182, train_loss: 0.3703, step time: 0.1163 sec\n",
      "Step 116/182, train_loss: 0.3681, step time: 0.1156 sec\n",
      "Step 117/182, train_loss: 0.3761, step time: 0.1156 sec\n",
      "Step 118/182, train_loss: 0.3788, step time: 0.1170 sec\n",
      "Step 119/182, train_loss: 0.3735, step time: 0.1168 sec\n",
      "Step 120/182, train_loss: 0.3724, step time: 0.1149 sec\n",
      "Step 121/182, train_loss: 0.3800, step time: 0.1152 sec\n",
      "Step 122/182, train_loss: 0.3746, step time: 0.1183 sec\n",
      "Step 123/182, train_loss: 0.3781, step time: 0.1140 sec\n",
      "Step 124/182, train_loss: 0.3826, step time: 0.1163 sec\n",
      "Step 125/182, train_loss: 0.3740, step time: 0.1162 sec\n",
      "Step 126/182, train_loss: 0.3712, step time: 0.1164 sec\n",
      "Step 127/182, train_loss: 0.3667, step time: 0.1148 sec\n",
      "Step 128/182, train_loss: 0.3736, step time: 0.1151 sec\n",
      "Step 129/182, train_loss: 0.3814, step time: 0.1166 sec\n",
      "Step 130/182, train_loss: 0.3813, step time: 0.1269 sec\n",
      "Step 131/182, train_loss: 0.3823, step time: 0.1152 sec\n",
      "Step 132/182, train_loss: 0.3672, step time: 0.1260 sec\n",
      "Step 133/182, train_loss: 0.4735, step time: 0.1339 sec\n",
      "Step 134/182, train_loss: 0.3699, step time: 0.1249 sec\n",
      "Step 135/182, train_loss: 0.3848, step time: 0.1224 sec\n",
      "Step 136/182, train_loss: 0.3737, step time: 0.1224 sec\n",
      "Step 137/182, train_loss: 0.3710, step time: 0.1193 sec\n",
      "Step 138/182, train_loss: 0.3765, step time: 0.1212 sec\n",
      "Step 139/182, train_loss: 0.3712, step time: 0.1189 sec\n",
      "Step 140/182, train_loss: 0.3652, step time: 0.1192 sec\n",
      "Step 141/182, train_loss: 0.3674, step time: 0.1218 sec\n",
      "Step 142/182, train_loss: 0.3748, step time: 0.1202 sec\n",
      "Step 143/182, train_loss: 0.3754, step time: 0.1220 sec\n",
      "Step 144/182, train_loss: 0.4929, step time: 0.1200 sec\n",
      "Step 145/182, train_loss: 0.3684, step time: 0.1196 sec\n",
      "Step 146/182, train_loss: 0.3778, step time: 0.1200 sec\n",
      "Step 147/182, train_loss: 0.3772, step time: 0.1212 sec\n",
      "Step 148/182, train_loss: 0.3804, step time: 0.1211 sec\n",
      "Step 149/182, train_loss: 0.3850, step time: 0.1207 sec\n",
      "Step 150/182, train_loss: 0.3733, step time: 0.1207 sec\n",
      "Step 151/182, train_loss: 0.3776, step time: 0.1208 sec\n",
      "Step 152/182, train_loss: 0.3756, step time: 0.1216 sec\n",
      "Step 153/182, train_loss: 0.3744, step time: 0.1215 sec\n",
      "Step 154/182, train_loss: 0.3674, step time: 0.1211 sec\n",
      "Step 155/182, train_loss: 0.3782, step time: 0.1197 sec\n",
      "Step 156/182, train_loss: 0.3792, step time: 0.1235 sec\n",
      "Step 157/182, train_loss: 0.3727, step time: 0.1199 sec\n",
      "Step 158/182, train_loss: 0.3799, step time: 0.1214 sec\n",
      "Step 159/182, train_loss: 0.3785, step time: 0.1209 sec\n",
      "Step 160/182, train_loss: 0.3623, step time: 0.1210 sec\n",
      "Step 161/182, train_loss: 0.3721, step time: 0.1210 sec\n",
      "Step 162/182, train_loss: 0.3753, step time: 0.1213 sec\n",
      "Step 163/182, train_loss: 0.3657, step time: 0.1211 sec\n",
      "Step 164/182, train_loss: 0.3738, step time: 0.1209 sec\n",
      "Step 165/182, train_loss: 0.3741, step time: 0.1206 sec\n",
      "Step 166/182, train_loss: 0.3748, step time: 0.1211 sec\n",
      "Step 167/182, train_loss: 0.3738, step time: 0.1212 sec\n",
      "Step 168/182, train_loss: 0.3782, step time: 0.1205 sec\n",
      "Step 169/182, train_loss: 0.4864, step time: 0.1221 sec\n",
      "Step 170/182, train_loss: 0.3698, step time: 0.1208 sec\n",
      "Step 171/182, train_loss: 0.3844, step time: 0.1205 sec\n",
      "Step 172/182, train_loss: 0.3615, step time: 0.1211 sec\n",
      "Step 173/182, train_loss: 0.3670, step time: 0.1219 sec\n",
      "Step 174/182, train_loss: 0.3722, step time: 0.1223 sec\n",
      "Step 175/182, train_loss: 0.3703, step time: 0.1223 sec\n",
      "Step 176/182, train_loss: 0.3779, step time: 0.1209 sec\n",
      "Step 177/182, train_loss: 0.3809, step time: 0.1207 sec\n",
      "Step 178/182, train_loss: 0.3756, step time: 0.1209 sec\n",
      "Step 179/182, train_loss: 0.3742, step time: 0.1208 sec\n",
      "Step 180/182, train_loss: 0.3723, step time: 0.1200 sec\n",
      "Step 181/182, train_loss: 0.3719, step time: 0.1202 sec\n",
      "Step 182/182, train_loss: 0.3448, step time: 0.0461 sec\n",
      "epoch 1 average loss: 0.3810\n",
      "Validation Loss: 0.3769\n",
      "Validation PSNR: 4.5698\n",
      "Saved new best model at epoch 1\n",
      "Epoch 1 validation metric: 4.5698, best metric so far: 4.5698 at epoch 1\n",
      "Time taken for epoch 1: 37.1115 seconds\n",
      "----------\n",
      "epoch 2/200\n",
      "Step 1/182, train_loss: 0.3635, step time: 0.1235 sec\n",
      "Step 2/182, train_loss: 0.3788, step time: 0.1234 sec\n",
      "Step 3/182, train_loss: 0.3588, step time: 0.1258 sec\n",
      "Step 4/182, train_loss: 0.3763, step time: 0.1254 sec\n",
      "Step 5/182, train_loss: 0.3664, step time: 0.1253 sec\n",
      "Step 6/182, train_loss: 0.3733, step time: 0.1249 sec\n",
      "Step 7/182, train_loss: 0.3675, step time: 0.1224 sec\n",
      "Step 8/182, train_loss: 0.3664, step time: 0.1241 sec\n",
      "Step 9/182, train_loss: 0.3689, step time: 0.1244 sec\n",
      "Step 10/182, train_loss: 0.3759, step time: 0.1239 sec\n",
      "Step 11/182, train_loss: 0.3684, step time: 0.1245 sec\n",
      "Step 12/182, train_loss: 0.3772, step time: 0.1248 sec\n",
      "Step 13/182, train_loss: 0.3642, step time: 0.1238 sec\n",
      "Step 14/182, train_loss: 0.3658, step time: 0.1246 sec\n",
      "Step 15/182, train_loss: 0.3636, step time: 0.1220 sec\n",
      "Step 16/182, train_loss: 0.3737, step time: 0.1216 sec\n",
      "Step 17/182, train_loss: 0.3632, step time: 0.1229 sec\n",
      "Step 18/182, train_loss: 0.3764, step time: 0.1228 sec\n",
      "Step 19/182, train_loss: 0.3700, step time: 0.1217 sec\n",
      "Step 20/182, train_loss: 0.3691, step time: 0.1219 sec\n",
      "Step 21/182, train_loss: 0.3722, step time: 0.1228 sec\n",
      "Step 22/182, train_loss: 0.3762, step time: 0.1223 sec\n",
      "Step 23/182, train_loss: 0.3805, step time: 0.1206 sec\n",
      "Step 24/182, train_loss: 0.3697, step time: 0.1214 sec\n",
      "Step 25/182, train_loss: 0.3797, step time: 0.1215 sec\n",
      "Step 26/182, train_loss: 0.3562, step time: 0.1211 sec\n",
      "Step 27/182, train_loss: 0.3762, step time: 0.1193 sec\n",
      "Step 28/182, train_loss: 0.3737, step time: 0.1203 sec\n",
      "Step 29/182, train_loss: 0.3821, step time: 0.1201 sec\n",
      "Step 30/182, train_loss: 0.3700, step time: 0.1244 sec\n",
      "Step 31/182, train_loss: 0.3717, step time: 0.1234 sec\n",
      "Step 32/182, train_loss: 0.3747, step time: 0.1195 sec\n",
      "Step 33/182, train_loss: 0.3690, step time: 0.1208 sec\n",
      "Step 34/182, train_loss: 0.3693, step time: 0.1207 sec\n",
      "Step 35/182, train_loss: 0.3697, step time: 0.1222 sec\n",
      "Step 36/182, train_loss: 0.3751, step time: 0.1218 sec\n",
      "Step 37/182, train_loss: 0.3670, step time: 0.1212 sec\n",
      "Step 38/182, train_loss: 0.3635, step time: 0.1231 sec\n",
      "Step 39/182, train_loss: 0.3783, step time: 0.1217 sec\n",
      "Step 40/182, train_loss: 0.3731, step time: 0.1220 sec\n",
      "Step 41/182, train_loss: 0.3774, step time: 0.1205 sec\n",
      "Step 42/182, train_loss: 0.3710, step time: 0.1211 sec\n",
      "Step 43/182, train_loss: 0.3712, step time: 0.1208 sec\n",
      "Step 44/182, train_loss: 0.3648, step time: 0.1205 sec\n",
      "Step 45/182, train_loss: 0.3706, step time: 0.1210 sec\n",
      "Step 46/182, train_loss: 0.3721, step time: 0.1197 sec\n",
      "Step 47/182, train_loss: 0.3811, step time: 0.1215 sec\n",
      "Step 48/182, train_loss: 0.3762, step time: 0.1200 sec\n",
      "Step 49/182, train_loss: 0.3748, step time: 0.1189 sec\n",
      "Step 50/182, train_loss: 0.3805, step time: 0.1204 sec\n",
      "Step 51/182, train_loss: 0.3738, step time: 0.1205 sec\n",
      "Step 52/182, train_loss: 0.3685, step time: 0.1188 sec\n",
      "Step 53/182, train_loss: 0.3713, step time: 0.1197 sec\n",
      "Step 54/182, train_loss: 0.3733, step time: 0.1188 sec\n",
      "Step 55/182, train_loss: 0.3795, step time: 0.1209 sec\n",
      "Step 56/182, train_loss: 0.3674, step time: 0.1180 sec\n",
      "Step 57/182, train_loss: 0.3781, step time: 0.1191 sec\n",
      "Step 58/182, train_loss: 0.3695, step time: 0.1204 sec\n",
      "Step 59/182, train_loss: 0.3721, step time: 0.1204 sec\n",
      "Step 60/182, train_loss: 0.3668, step time: 0.1200 sec\n",
      "Step 61/182, train_loss: 0.3736, step time: 0.1192 sec\n",
      "Step 62/182, train_loss: 0.3747, step time: 0.1192 sec\n",
      "Step 63/182, train_loss: 0.3778, step time: 0.1211 sec\n",
      "Step 64/182, train_loss: 0.3671, step time: 0.1189 sec\n",
      "Step 65/182, train_loss: 0.4830, step time: 0.1202 sec\n",
      "Step 66/182, train_loss: 0.3592, step time: 0.1213 sec\n",
      "Step 67/182, train_loss: 0.3675, step time: 0.1224 sec\n",
      "Step 68/182, train_loss: 0.3760, step time: 0.1214 sec\n",
      "Step 69/182, train_loss: 0.3660, step time: 0.1211 sec\n",
      "Step 70/182, train_loss: 0.3677, step time: 0.1211 sec\n",
      "Step 71/182, train_loss: 0.3803, step time: 0.1215 sec\n",
      "Step 72/182, train_loss: 0.3772, step time: 0.1197 sec\n",
      "Step 73/182, train_loss: 0.3791, step time: 0.1205 sec\n",
      "Step 74/182, train_loss: 0.3638, step time: 0.1202 sec\n",
      "Step 75/182, train_loss: 0.3736, step time: 0.1212 sec\n",
      "Step 76/182, train_loss: 0.3669, step time: 0.1212 sec\n",
      "Step 77/182, train_loss: 0.3786, step time: 0.1210 sec\n",
      "Step 78/182, train_loss: 0.3781, step time: 0.1214 sec\n",
      "Step 79/182, train_loss: 0.3720, step time: 0.1217 sec\n",
      "Step 80/182, train_loss: 0.3788, step time: 0.1214 sec\n",
      "Step 81/182, train_loss: 0.4860, step time: 0.1204 sec\n",
      "Step 82/182, train_loss: 0.3686, step time: 0.1205 sec\n",
      "Step 83/182, train_loss: 0.3723, step time: 0.1215 sec\n",
      "Step 84/182, train_loss: 0.3747, step time: 0.1212 sec\n",
      "Step 85/182, train_loss: 0.3792, step time: 0.1209 sec\n",
      "Step 86/182, train_loss: 0.3643, step time: 0.1199 sec\n",
      "Step 87/182, train_loss: 0.3737, step time: 0.1204 sec\n",
      "Step 88/182, train_loss: 0.3767, step time: 0.1221 sec\n",
      "Step 89/182, train_loss: 0.3699, step time: 0.1214 sec\n",
      "Step 90/182, train_loss: 0.3698, step time: 0.1210 sec\n",
      "Step 91/182, train_loss: 0.3566, step time: 0.1226 sec\n",
      "Step 92/182, train_loss: 0.3667, step time: 0.1205 sec\n",
      "Step 93/182, train_loss: 0.3681, step time: 0.1208 sec\n",
      "Step 94/182, train_loss: 0.3585, step time: 0.1220 sec\n",
      "Step 95/182, train_loss: 0.3677, step time: 0.1205 sec\n",
      "Step 96/182, train_loss: 0.3762, step time: 0.1217 sec\n",
      "Step 97/182, train_loss: 0.3572, step time: 0.1206 sec\n",
      "Step 98/182, train_loss: 0.3719, step time: 0.1210 sec\n",
      "Step 99/182, train_loss: 0.3711, step time: 0.1210 sec\n",
      "Step 100/182, train_loss: 0.3678, step time: 0.1208 sec\n",
      "Step 101/182, train_loss: 0.4829, step time: 0.1222 sec\n",
      "Step 102/182, train_loss: 0.3590, step time: 0.1207 sec\n",
      "Step 103/182, train_loss: 0.3650, step time: 0.1209 sec\n",
      "Step 104/182, train_loss: 0.4820, step time: 0.1203 sec\n",
      "Step 105/182, train_loss: 0.3723, step time: 0.1218 sec\n",
      "Step 106/182, train_loss: 0.3713, step time: 0.1211 sec\n",
      "Step 107/182, train_loss: 0.3698, step time: 0.1204 sec\n",
      "Step 108/182, train_loss: 0.3696, step time: 0.1206 sec\n",
      "Step 109/182, train_loss: 0.3771, step time: 0.1212 sec\n",
      "Step 110/182, train_loss: 0.3633, step time: 0.1214 sec\n",
      "Step 111/182, train_loss: 0.3755, step time: 0.1210 sec\n",
      "Step 112/182, train_loss: 0.3753, step time: 0.1205 sec\n",
      "Step 113/182, train_loss: 0.3693, step time: 0.1217 sec\n",
      "Step 114/182, train_loss: 0.3745, step time: 0.1216 sec\n",
      "Step 115/182, train_loss: 0.3721, step time: 0.1206 sec\n",
      "Step 116/182, train_loss: 0.3776, step time: 0.1209 sec\n",
      "Step 117/182, train_loss: 0.3679, step time: 0.1252 sec\n",
      "Step 118/182, train_loss: 0.4788, step time: 0.1237 sec\n",
      "Step 119/182, train_loss: 0.3638, step time: 0.1243 sec\n",
      "Step 120/182, train_loss: 0.3767, step time: 0.1242 sec\n",
      "Step 121/182, train_loss: 0.3697, step time: 0.1230 sec\n",
      "Step 122/182, train_loss: 0.3628, step time: 0.1241 sec\n",
      "Step 123/182, train_loss: 0.3705, step time: 0.1233 sec\n",
      "Step 124/182, train_loss: 0.3785, step time: 0.1261 sec\n",
      "Step 125/182, train_loss: 0.3683, step time: 0.1250 sec\n",
      "Step 126/182, train_loss: 0.3744, step time: 0.1242 sec\n",
      "Step 127/182, train_loss: 0.3730, step time: 0.1243 sec\n",
      "Step 128/182, train_loss: 0.3782, step time: 0.1259 sec\n",
      "Step 129/182, train_loss: 0.3673, step time: 0.1247 sec\n",
      "Step 130/182, train_loss: 0.3637, step time: 0.1265 sec\n",
      "Step 131/182, train_loss: 0.3681, step time: 0.1257 sec\n",
      "Step 132/182, train_loss: 0.3651, step time: 0.1240 sec\n",
      "Step 133/182, train_loss: 0.3748, step time: 0.1255 sec\n",
      "Step 134/182, train_loss: 0.3740, step time: 0.1253 sec\n",
      "Step 135/182, train_loss: 0.3608, step time: 0.1254 sec\n",
      "Step 136/182, train_loss: 0.3643, step time: 0.1261 sec\n",
      "Step 137/182, train_loss: 0.3801, step time: 0.1232 sec\n",
      "Step 138/182, train_loss: 0.3618, step time: 0.1236 sec\n",
      "Step 139/182, train_loss: 0.3719, step time: 0.1233 sec\n",
      "Step 140/182, train_loss: 0.3701, step time: 0.1233 sec\n",
      "Step 141/182, train_loss: 0.3720, step time: 0.1256 sec\n",
      "Step 142/182, train_loss: 0.3633, step time: 0.1271 sec\n",
      "Step 143/182, train_loss: 0.3737, step time: 0.1252 sec\n",
      "Step 144/182, train_loss: 0.3701, step time: 0.1263 sec\n",
      "Step 145/182, train_loss: 0.3728, step time: 0.1266 sec\n",
      "Step 146/182, train_loss: 0.3698, step time: 0.1260 sec\n",
      "Step 147/182, train_loss: 0.3720, step time: 0.1261 sec\n",
      "Step 148/182, train_loss: 0.3727, step time: 0.1251 sec\n",
      "Step 149/182, train_loss: 0.3666, step time: 0.1257 sec\n",
      "Step 150/182, train_loss: 0.3696, step time: 0.1251 sec\n",
      "Step 151/182, train_loss: 0.3645, step time: 0.1251 sec\n",
      "Step 152/182, train_loss: 0.3715, step time: 0.1234 sec\n",
      "Step 153/182, train_loss: 0.3666, step time: 0.1229 sec\n",
      "Step 154/182, train_loss: 0.3752, step time: 0.1231 sec\n",
      "Step 155/182, train_loss: 0.3673, step time: 0.1243 sec\n",
      "Step 156/182, train_loss: 0.3692, step time: 0.1263 sec\n",
      "Step 157/182, train_loss: 0.3702, step time: 0.1246 sec\n",
      "Step 158/182, train_loss: 0.3683, step time: 0.1272 sec\n",
      "Step 159/182, train_loss: 0.3708, step time: 0.1261 sec\n",
      "Step 160/182, train_loss: 0.3613, step time: 0.1256 sec\n",
      "Step 161/182, train_loss: 0.3620, step time: 0.1250 sec\n",
      "Step 162/182, train_loss: 0.3622, step time: 0.1251 sec\n",
      "Step 163/182, train_loss: 0.3621, step time: 0.1235 sec\n",
      "Step 164/182, train_loss: 0.3710, step time: 0.1252 sec\n",
      "Step 165/182, train_loss: 0.3686, step time: 0.1264 sec\n",
      "Step 166/182, train_loss: 0.3725, step time: 0.1239 sec\n",
      "Step 167/182, train_loss: 0.3690, step time: 0.1245 sec\n",
      "Step 168/182, train_loss: 0.3652, step time: 0.1279 sec\n",
      "Step 169/182, train_loss: 0.3673, step time: 0.1252 sec\n",
      "Step 170/182, train_loss: 0.3735, step time: 0.1234 sec\n",
      "Step 171/182, train_loss: 0.3722, step time: 0.1258 sec\n",
      "Step 172/182, train_loss: 0.3675, step time: 0.1260 sec\n",
      "Step 173/182, train_loss: 0.3634, step time: 0.1244 sec\n",
      "Step 174/182, train_loss: 0.3688, step time: 0.1258 sec\n",
      "Step 175/182, train_loss: 0.3704, step time: 0.1256 sec\n",
      "Step 176/182, train_loss: 0.3635, step time: 0.1239 sec\n",
      "Step 177/182, train_loss: 0.3746, step time: 0.1093 sec\n",
      "Step 178/182, train_loss: 0.3660, step time: 0.1105 sec\n",
      "Step 179/182, train_loss: 0.3674, step time: 0.1100 sec\n",
      "Step 180/182, train_loss: 0.3625, step time: 0.1094 sec\n",
      "Step 181/182, train_loss: 0.3693, step time: 0.1092 sec\n",
      "Step 182/182, train_loss: 0.6920, step time: 0.0476 sec\n",
      "epoch 2 average loss: 0.3752\n",
      "Validation Loss: 0.3722\n",
      "Validation PSNR: 4.6254\n",
      "Saved new best model at epoch 2\n",
      "Epoch 2 validation metric: 4.6254, best metric so far: 4.6254 at epoch 2\n",
      "Time taken for epoch 2: 36.9280 seconds\n",
      "----------\n",
      "epoch 3/200\n",
      "Step 1/182, train_loss: 0.3651, step time: 0.1226 sec\n",
      "Step 2/182, train_loss: 0.3728, step time: 0.1223 sec\n",
      "Step 3/182, train_loss: 0.3574, step time: 0.1212 sec\n",
      "Step 4/182, train_loss: 0.3656, step time: 0.1126 sec\n",
      "Step 5/182, train_loss: 0.4736, step time: 0.1218 sec\n",
      "Step 6/182, train_loss: 0.3642, step time: 0.1200 sec\n",
      "Step 7/182, train_loss: 0.3750, step time: 0.1211 sec\n",
      "Step 8/182, train_loss: 0.3649, step time: 0.1203 sec\n",
      "Step 9/182, train_loss: 0.3689, step time: 0.1211 sec\n",
      "Step 10/182, train_loss: 0.3742, step time: 0.1116 sec\n",
      "Step 11/182, train_loss: 0.3579, step time: 0.1091 sec\n",
      "Step 12/182, train_loss: 0.3600, step time: 0.1105 sec\n",
      "Step 13/182, train_loss: 0.3718, step time: 0.1092 sec\n",
      "Step 14/182, train_loss: 0.3744, step time: 0.1092 sec\n",
      "Step 15/182, train_loss: 0.3602, step time: 0.1172 sec\n",
      "Step 16/182, train_loss: 0.3650, step time: 0.1151 sec\n",
      "Step 17/182, train_loss: 0.3664, step time: 0.1130 sec\n",
      "Step 18/182, train_loss: 0.3682, step time: 0.1210 sec\n",
      "Step 19/182, train_loss: 0.3717, step time: 0.1244 sec\n",
      "Step 20/182, train_loss: 0.3636, step time: 0.1244 sec\n",
      "Step 21/182, train_loss: 0.3793, step time: 0.1231 sec\n",
      "Step 22/182, train_loss: 0.3677, step time: 0.1227 sec\n",
      "Step 23/182, train_loss: 0.3663, step time: 0.1249 sec\n",
      "Step 24/182, train_loss: 0.3655, step time: 0.1259 sec\n",
      "Step 25/182, train_loss: 0.3697, step time: 0.1164 sec\n",
      "Step 26/182, train_loss: 0.3652, step time: 0.1152 sec\n",
      "Step 27/182, train_loss: 0.3624, step time: 0.1146 sec\n",
      "Step 28/182, train_loss: 0.3664, step time: 0.1099 sec\n",
      "Step 29/182, train_loss: 0.3745, step time: 0.1093 sec\n",
      "Step 30/182, train_loss: 0.3636, step time: 0.1127 sec\n",
      "Step 31/182, train_loss: 0.3700, step time: 0.1091 sec\n",
      "Step 32/182, train_loss: 0.3590, step time: 0.1093 sec\n",
      "Step 33/182, train_loss: 0.3610, step time: 0.1120 sec\n",
      "Step 34/182, train_loss: 0.3694, step time: 0.1092 sec\n",
      "Step 35/182, train_loss: 0.3650, step time: 0.1093 sec\n",
      "Step 36/182, train_loss: 0.3787, step time: 0.1121 sec\n",
      "Step 37/182, train_loss: 0.3580, step time: 0.1094 sec\n",
      "Step 38/182, train_loss: 0.3720, step time: 0.1094 sec\n",
      "Step 39/182, train_loss: 0.3696, step time: 0.1100 sec\n",
      "Step 40/182, train_loss: 0.3761, step time: 0.1092 sec\n",
      "Step 41/182, train_loss: 0.3728, step time: 0.1094 sec\n",
      "Step 42/182, train_loss: 0.3767, step time: 0.1100 sec\n",
      "Step 43/182, train_loss: 0.3596, step time: 0.1099 sec\n",
      "Step 44/182, train_loss: 0.3759, step time: 0.1095 sec\n",
      "Step 45/182, train_loss: 0.3738, step time: 0.1114 sec\n",
      "Step 46/182, train_loss: 0.3615, step time: 0.1092 sec\n",
      "Step 47/182, train_loss: 0.3687, step time: 0.1136 sec\n",
      "Step 48/182, train_loss: 0.3531, step time: 0.1154 sec\n",
      "Step 49/182, train_loss: 0.3663, step time: 0.1117 sec\n",
      "Step 50/182, train_loss: 0.3680, step time: 0.1092 sec\n",
      "Step 51/182, train_loss: 0.3693, step time: 0.1225 sec\n",
      "Step 52/182, train_loss: 0.3701, step time: 0.1093 sec\n",
      "Step 53/182, train_loss: 0.3648, step time: 0.1112 sec\n",
      "Step 54/182, train_loss: 0.3692, step time: 0.1171 sec\n",
      "Step 55/182, train_loss: 0.3721, step time: 0.1114 sec\n",
      "Step 56/182, train_loss: 0.3713, step time: 0.1118 sec\n",
      "Step 57/182, train_loss: 0.3508, step time: 0.1127 sec\n",
      "Step 58/182, train_loss: 0.4828, step time: 0.1239 sec\n",
      "Step 59/182, train_loss: 0.3653, step time: 0.1220 sec\n",
      "Step 60/182, train_loss: 0.3639, step time: 0.1216 sec\n",
      "Step 61/182, train_loss: 0.3641, step time: 0.1207 sec\n",
      "Step 62/182, train_loss: 0.3701, step time: 0.1210 sec\n",
      "Step 63/182, train_loss: 0.3654, step time: 0.1161 sec\n",
      "Step 64/182, train_loss: 0.3641, step time: 0.1093 sec\n",
      "Step 65/182, train_loss: 0.3591, step time: 0.1170 sec\n",
      "Step 66/182, train_loss: 0.3657, step time: 0.1094 sec\n",
      "Step 67/182, train_loss: 0.3724, step time: 0.1118 sec\n",
      "Step 68/182, train_loss: 0.3703, step time: 0.1113 sec\n",
      "Step 69/182, train_loss: 0.3685, step time: 0.1095 sec\n",
      "Step 70/182, train_loss: 0.3563, step time: 0.1197 sec\n",
      "Step 71/182, train_loss: 0.3690, step time: 0.1129 sec\n",
      "Step 72/182, train_loss: 0.3656, step time: 0.1134 sec\n",
      "Step 73/182, train_loss: 0.3766, step time: 0.1204 sec\n",
      "Step 74/182, train_loss: 0.3686, step time: 0.1225 sec\n",
      "Step 75/182, train_loss: 0.3638, step time: 0.1215 sec\n",
      "Step 76/182, train_loss: 0.3709, step time: 0.1207 sec\n",
      "Step 77/182, train_loss: 0.3756, step time: 0.1112 sec\n",
      "Step 78/182, train_loss: 0.3604, step time: 0.1117 sec\n",
      "Step 79/182, train_loss: 0.3651, step time: 0.1150 sec\n",
      "Step 80/182, train_loss: 0.3738, step time: 0.1123 sec\n",
      "Step 81/182, train_loss: 0.3675, step time: 0.1148 sec\n",
      "Step 82/182, train_loss: 0.4737, step time: 0.1098 sec\n",
      "Step 83/182, train_loss: 0.3610, step time: 0.1188 sec\n",
      "Step 84/182, train_loss: 0.3606, step time: 0.1093 sec\n",
      "Step 85/182, train_loss: 0.4667, step time: 0.1182 sec\n",
      "Step 86/182, train_loss: 0.3695, step time: 0.1093 sec\n",
      "Step 87/182, train_loss: 0.3678, step time: 0.1120 sec\n",
      "Step 88/182, train_loss: 0.3679, step time: 0.1119 sec\n",
      "Step 89/182, train_loss: 0.3746, step time: 0.1221 sec\n",
      "Step 90/182, train_loss: 0.3631, step time: 0.1205 sec\n",
      "Step 91/182, train_loss: 0.3553, step time: 0.1207 sec\n",
      "Step 92/182, train_loss: 0.3793, step time: 0.1255 sec\n",
      "Step 93/182, train_loss: 0.3706, step time: 0.1153 sec\n",
      "Step 94/182, train_loss: 0.3647, step time: 0.1134 sec\n",
      "Step 95/182, train_loss: 0.3661, step time: 0.1243 sec\n",
      "Step 96/182, train_loss: 0.3752, step time: 0.1212 sec\n",
      "Step 97/182, train_loss: 0.3730, step time: 0.1213 sec\n",
      "Step 98/182, train_loss: 0.3544, step time: 0.1213 sec\n",
      "Step 99/182, train_loss: 0.3643, step time: 0.1216 sec\n",
      "Step 100/182, train_loss: 0.3658, step time: 0.1095 sec\n",
      "Step 101/182, train_loss: 0.3726, step time: 0.1092 sec\n",
      "Step 102/182, train_loss: 0.3633, step time: 0.1092 sec\n",
      "Step 103/182, train_loss: 0.3673, step time: 0.1095 sec\n",
      "Step 104/182, train_loss: 0.4638, step time: 0.1231 sec\n",
      "Step 105/182, train_loss: 0.3517, step time: 0.1172 sec\n",
      "Step 106/182, train_loss: 0.3667, step time: 0.1117 sec\n",
      "Step 107/182, train_loss: 0.3693, step time: 0.1092 sec\n",
      "Step 108/182, train_loss: 0.3689, step time: 0.1092 sec\n",
      "Step 109/182, train_loss: 0.3701, step time: 0.1094 sec\n",
      "Step 110/182, train_loss: 0.3602, step time: 0.1090 sec\n",
      "Step 111/182, train_loss: 0.3632, step time: 0.1092 sec\n",
      "Step 112/182, train_loss: 0.3589, step time: 0.1094 sec\n",
      "Step 113/182, train_loss: 0.3570, step time: 0.1092 sec\n",
      "Step 114/182, train_loss: 0.3608, step time: 0.1092 sec\n",
      "Step 115/182, train_loss: 0.3701, step time: 0.1091 sec\n",
      "Step 116/182, train_loss: 0.3659, step time: 0.1092 sec\n",
      "Step 117/182, train_loss: 0.3746, step time: 0.1092 sec\n",
      "Step 118/182, train_loss: 0.3590, step time: 0.1093 sec\n",
      "Step 119/182, train_loss: 0.3671, step time: 0.1210 sec\n",
      "Step 120/182, train_loss: 0.3765, step time: 0.1102 sec\n",
      "Step 121/182, train_loss: 0.3688, step time: 0.1193 sec\n",
      "Step 122/182, train_loss: 0.3600, step time: 0.1094 sec\n",
      "Step 123/182, train_loss: 0.3577, step time: 0.1236 sec\n",
      "Step 124/182, train_loss: 0.4747, step time: 0.1094 sec\n",
      "Step 125/182, train_loss: 0.3718, step time: 0.1160 sec\n",
      "Step 126/182, train_loss: 0.3668, step time: 0.1093 sec\n",
      "Step 127/182, train_loss: 0.3516, step time: 0.1136 sec\n",
      "Step 128/182, train_loss: 0.3753, step time: 0.1196 sec\n",
      "Step 129/182, train_loss: 0.3711, step time: 0.1216 sec\n",
      "Step 130/182, train_loss: 0.3665, step time: 0.1214 sec\n",
      "Step 131/182, train_loss: 0.3589, step time: 0.1211 sec\n",
      "Step 132/182, train_loss: 0.3675, step time: 0.1146 sec\n",
      "Step 133/182, train_loss: 0.3601, step time: 0.1125 sec\n",
      "Step 134/182, train_loss: 0.3678, step time: 0.1099 sec\n",
      "Step 135/182, train_loss: 0.3716, step time: 0.1116 sec\n",
      "Step 136/182, train_loss: 0.3650, step time: 0.1091 sec\n",
      "Step 137/182, train_loss: 0.3565, step time: 0.1140 sec\n",
      "Step 138/182, train_loss: 0.3726, step time: 0.1093 sec\n",
      "Step 139/182, train_loss: 0.3742, step time: 0.1128 sec\n",
      "Step 140/182, train_loss: 0.3565, step time: 0.1095 sec\n",
      "Step 141/182, train_loss: 0.3580, step time: 0.1092 sec\n",
      "Step 142/182, train_loss: 0.3683, step time: 0.1095 sec\n",
      "Step 143/182, train_loss: 0.3705, step time: 0.1239 sec\n",
      "Step 144/182, train_loss: 0.3619, step time: 0.1207 sec\n",
      "Step 145/182, train_loss: 0.3658, step time: 0.1186 sec\n",
      "Step 146/182, train_loss: 0.3629, step time: 0.1189 sec\n",
      "Step 147/182, train_loss: 0.3747, step time: 0.1180 sec\n",
      "Step 148/182, train_loss: 0.3530, step time: 0.1159 sec\n",
      "Step 149/182, train_loss: 0.3605, step time: 0.1157 sec\n",
      "Step 150/182, train_loss: 0.3703, step time: 0.1140 sec\n",
      "Step 151/182, train_loss: 0.3671, step time: 0.1199 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 148\u001b[0m\n\u001b[1;32m    145\u001b[0m     model_summary(model, device)\n\u001b[1;32m    146\u001b[0m     setup_training(model, device, train_loader, val_loader)\n\u001b[0;32m--> 148\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 146\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m dataloaders(train_dataset, val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m    145\u001b[0m model_summary(model, device)\n\u001b[0;32m--> 146\u001b[0m \u001b[43msetup_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m, in \u001b[0;36msetup_training\u001b[0;34m(model, device, train_loader, val_loader, epochs, patience)\u001b[0m\n\u001b[1;32m     76\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m CharbonnierLoss(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m     77\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model_output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify the output directory\u001b[39;49;00m\n\u001b[1;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Abul Hasan/Resolution-Enhancement-of-Solar-Magnetograms-main/Trainning_Loop.py:77\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, loss_function, device, epochs, patience, val_interval, lr_scheduler, output_dir)\u001b[0m\n\u001b[1;32m     74\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(lr_inputs)\n\u001b[1;32m     75\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, hr_targets)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/abul/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/abul/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available!\")\n",
    "        print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = MESR(in_channels=3, mid_channels=64, out_channels=3, num_blocks=16)\n",
    "\n",
    "    transform_lr = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    transform_hr = transforms.Compose([\n",
    "        transforms.Resize((256, 256)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    # load the dataset\n",
    "    lr_dir = \"/home/user/Desktop/Abul Hasan/Dataset/LRHR dataset/renamedsoho\"\n",
    "    hr_dir = \"/home/user/Desktop/Abul Hasan/Dataset/LRHR dataset/renamedsdo\"\n",
    "\n",
    "    # dataset instance with separate transformations for LR and HR images\n",
    "    full_dataset = SuperResolutionDataset(\n",
    "        lr_dir=lr_dir,\n",
    "        hr_dir=hr_dir,\n",
    "        lr_transform=transform_lr,\n",
    "        hr_transform=transform_hr\n",
    "    )\n",
    "\n",
    "    #splitting the dataset\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    train_loader, val_loader = dataloaders(train_dataset, val_dataset, batch_size=6)\n",
    "\n",
    "    model_summary(model, device)\n",
    "    setup_training(model, device, train_loader, val_loader)\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
