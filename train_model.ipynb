{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import random\n",
    "\n",
    "\n",
    "from conv_layer import conv_layer\n",
    "from RLFB import RLFB\n",
    "from SUBP import SubPixelConvBlock\n",
    "from Trainning_Loop import train_model, CharbonnierLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MESR(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, num_blocks=12, esa_channels=32, upscale_factor=32):\n",
    "        super(MESR, self).__init__()\n",
    "\n",
    "\n",
    "        self.conv_in = conv_layer(in_channels, mid_channels, 3)\n",
    "\n",
    "        self.RLFB_blocks = nn.Sequential(*[RLFB(mid_channels, esa_channels=esa_channels) for _ in range(num_blocks)])\n",
    "\n",
    "        self.conv_out1 = conv_layer(mid_channels, out_channels, 3)\n",
    "\n",
    "        self.conv_out2 = conv_layer(mid_channels, out_channels, 3)\n",
    "\n",
    "        self.sub_pixel_conv = SubPixelConvBlock(out_channels, out_channels, upscale_factor=upscale_factor)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out_conv_in = self.conv_in(x)  \n",
    "\n",
    "        out_RLFB = self.RLFB_blocks(out_conv_in)\n",
    "\n",
    "        out2_conv_in = self.conv_out1(out_RLFB)  \n",
    "\n",
    "        out_skip = out2_conv_in + out_conv_in  \n",
    "\n",
    "        out = self.conv_out2(out_skip)  \n",
    "\n",
    "        out = self.sub_pixel_conv(out)  \n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_summary(model, device):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    summary(model, input_size=(3, 128, 128)) # Change order & num of channels to match grayscale channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MESR(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, num_blocks=12, esa_channels=16, upscale_factor=32):\n",
    "        super(MESR, self).__init__()\n",
    "\n",
    "        self.conv_in = conv_layer(in_channels, mid_channels, 3)\n",
    "        self.RLFB_blocks = nn.Sequential(*[RLFB(mid_channels, esa_channels=esa_channels) for _ in range(num_blocks)])\n",
    "        self.conv_out = conv_layer(mid_channels, out_channels, 3)\n",
    "        self.sub_pixel_conv = SubPixelConvBlock(out_channels, out_channels, upscale_factor=upscale_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_conv_in = self.conv_in(x)  \n",
    "        out_RLFB = self.RLFB_blocks(out_conv_in)  \n",
    "        out_skip = out_RLFB + out_conv_in  \n",
    "        out = self.conv_out(out_skip)  \n",
    "        out = self.sub_pixel_conv(out)  \n",
    "        return out\n",
    "\n",
    "\n",
    "def model_summary(model, device):\n",
    "    model.to(device)\n",
    "    summary(model, input_size=(3, 128, 128)) # Change order & num of channels to match grayscale channel \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, lr_transform=None, hr_transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_images = os.listdir(lr_dir)\n",
    "        self.lr_transform = lr_transform\n",
    "        self.hr_transform = hr_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_image_path = os.path.join(self.lr_dir, self.lr_images[idx])\n",
    "        hr_image_path = os.path.join(self.hr_dir, self.lr_images[idx])\n",
    "\n",
    "        lr_image = Image.open(lr_image_path).convert(\"RGB\")\n",
    "        hr_image = Image.open(hr_image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.lr_transform:\n",
    "            lr_image = self.lr_transform(lr_image)\n",
    "        if self.hr_transform:\n",
    "            hr_image = self.hr_transform(hr_image)\n",
    "\n",
    "        return {'image': lr_image, 'label': hr_image}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloaders(train_dataset, val_dataset, batch_size=1): # setting the batch size to 2\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def setup_training(model, device, train_loader_func, val_loader, stages, epochs=50, patience=50):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    loss_function = CharbonnierLoss(epsilon=1e-6)\n",
    "\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader_func=train_loader_func,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss_function,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        val_interval=10,\n",
    "        output_dir=\"./model_output\",  # Specify the output directory\n",
    "        stages =stages\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is being used. Device: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is being used. Device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU instead of GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of available GPUs: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m     model_summary(model, device)\n\u001b[0;32m     48\u001b[0m     setup_training(model, device, train_loader, val_loader)\n\u001b[1;32m---> 49\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 47\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m random_split(full_dataset, [train_size, val_size])\n\u001b[0;32m     43\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m dataloaders(train_dataset, val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mmodel_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m setup_training(model, device, train_loader, val_loader)\n",
      "Cell \u001b[1;32mIn[8], line 42\u001b[0m, in \u001b[0;36mmodel_summary\u001b[1;34m(model, device)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_summary\u001b[39m(model, device):\n\u001b[0;32m     40\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 42\u001b[0m     \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abul4\\miniforge3\\envs\\pytorch2\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\abul4\\miniforge3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abul4\\miniforge3\\envs\\pytorch2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m, in \u001b[0;36mMESR.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m out_RLFB \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRLFB_blocks(out_conv_in)\n\u001b[0;32m     24\u001b[0m out2_conv_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_out1(out_RLFB)  \n\u001b[1;32m---> 26\u001b[0m out_skip \u001b[38;5;241m=\u001b[39m \u001b[43mout2_conv_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout_conv_in\u001b[49m  \n\u001b[0;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_out2(out_skip)  \n\u001b[0;32m     30\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_pixel_conv(out)  \n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available!\")\n",
    "        print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = MESR(in_channels=3, mid_channels=64, out_channels=3, num_blocks=12)\n",
    "\n",
    "    transform_lr = transforms.Compose([\n",
    " \n",
    "        transforms.ToTensor(),\n",
    "\n",
    "    ])\n",
    "\n",
    "    transform_hr = transforms.Compose([\n",
    "\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "    ])\n",
    "\n",
    "    # load the dataset\n",
    "    lr_dir = \"low_res\"\n",
    "    hr_dir = \"high_res\"\n",
    "\n",
    "    # dataset instance with separate transformations for LR and HR images\n",
    "    full_dataset = SuperResolutionDataset(\n",
    "        lr_dir=lr_dir,\n",
    "        hr_dir=hr_dir,\n",
    "        lr_transform=transform_lr,\n",
    "        hr_transform=transform_hr\n",
    "    )\n",
    "\n",
    "    #splitting the dataset\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    train_loader, val_loader = dataloaders(train_dataset, val_dataset, batch_size=2)\n",
    "\n",
    "\n",
    "\n",
    "    model_summary(model, device)\n",
    "    setup_training(model, device, train_loader, val_loader)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
