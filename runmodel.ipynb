{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from MESR_TEST import MESR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, device):\n",
    "    model = MESR(in_channels=3, mid_channels=64, out_channels=3, num_blocks=12)\n",
    "    # loading the weights of the model \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    # evaluation mode to use the fixed running statistics\n",
    "    model.eval()  \n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "def preprocess_image(image_path, device):\n",
    "    # stages of preprocessing used in the training\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  \n",
    "        transforms.ToTensor(),      \n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "    ])\n",
    "    \n",
    "    # loading and preprocessing\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    return input_tensor.to(device), image\n",
    "\n",
    "def postprocess_output(output_tensor):\n",
    "    # Undo normalization and convert to image\n",
    "    output_tensor = output_tensor.squeeze(0).cpu().detach()\n",
    "    # denormalization of the output tensor\n",
    "    output_tensor = output_tensor * 0.5 + 0.5  \n",
    "    output_image = transforms.ToPILImage()(output_tensor)\n",
    "    return output_image\n",
    "\n",
    "def visualize_results(input_image, enhanced_image):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # display input\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Low-Resolution Input\")\n",
    "    plt.imshow(input_image)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # display output\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"High-Resolution Enhanced\")\n",
    "    plt.imshow(enhanced_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def test_model(model_path, image_path, device): \n",
    "    \"\"\" load the model, preprocess the patch, run inference on the input tensor and postprocess \"\"\"\n",
    "    model = load_model(model_path, device)\n",
    "    input_tensor, input_image = preprocess_image(image_path, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tensor = model(input_tensor)\n",
    "\n",
    "    enhanced_image = postprocess_output(output_tensor)\n",
    "    visualize_results(input_image, enhanced_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Flow looks like this:\n",
    "instantiate the model -> set in the eval mode to load the training weights -> apply transformations on the input -> \n",
    "\n",
    "preprocess_image() would yeild input tensor and image -> feed the tensor in model to get output tensor -> postprocess ->\n",
    "\n",
    "get the output image -> display alongside the input image\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model_path = \"./model_output/best_model.pth\"\n",
    "    image_path = \"./sample_image.jpg\" \n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    test_model(model_path, image_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
